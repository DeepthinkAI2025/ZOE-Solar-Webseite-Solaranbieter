# Automations & Tooling

Die SEO- und Content-Automationen beschleunigen wiederkehrende Aufgaben und sichern konsistente DatenaktualitÃ¤t. Dieser Artikel beschreibt verfÃ¼gbare Scripts, deren Konfiguration sowie Betriebsrichtlinien.

## Ãœbersicht

| Kategorie | Script | Beschreibung |
| --- | --- | --- |
| Wiki & Dokumentation | `scripts/wiki-sync.ts` | Synchronisiert Markdown-Dokumente aus `docs/wiki` mit GitHub Wiki |
| SEO Monitoring | `scripts/seo-monitor.ts`, `scripts/seo-diff.ts` | Vergleicht Search Console/KPIs, erzeugt Alerts |
| Keyword Research | `scripts/keyword-cluster.ts`, `scripts/local-keywords.ts` | Aggregiert Keywords, bildet Cluster, aktualisiert JSON |
| Content Pipeline | `scripts/content-pipeline.ts` | Generiert Briefings, ordnet Content-Plan den Autoren zu |
| Sitemap & Indexierung | `scripts/generate-sitemap.ts`, `scripts/ping-search-engines.ts` | Baut XML-Sitemaps, pingt Google/Bing |
| Datenvalidierung | `scripts/validate-data.ts` (Backlog) | Stellt sicher, dass JSON/TS Daten konsistent sind |
| Build & Deployment | npm Scripts (`build`, `preview`, `lint`) | Frontend-Build und Preview |

## Wiki Sync

- **Zweck:** Markdown-Dateien automatisch mit GitHub Wiki Repository synchron halten.
- **Konfiguration:** `.env` benÃ¶tigt `GITHUB_WIKI_REMOTE` (SSH/HTTPS) sowie `WIKI_BRANCH` (Standard: `main`).
- **Nutzung:**
  1. Ã„nderungen in `docs/wiki` committen.
  2. `npm run wiki-sync` ausfÃ¼hren.
  3. Script clont/updatet Wiki-Repo in `.cache/wiki/`, kopiert Dateien und pusht.
- **Fehlerbehandlung:**
  - Netzwerkfehler â†’ Retry (max. 3 Versuche).
  - Merge-Konflikt â†’ lokales Wiki-Repo prÃ¼fen (`.cache/wiki`), `git status` laufen lassen, manuell beheben.

## SEO Monitoring

- Nutzt Google Search Console Bulk Exports + Ahrefs API.
- Generiert tÃ¤gliche JSONs in `data/seo-monitoring/`.
- Alert-Logik: 
  - Drop > 20 % Klicks Woche/Woche.
  - Neue Coverage Issues â†’ Slack Webhook (`SLACK_ALERT_URL`).
- geplanter Cron: `0 6 * * *` (GCP Cloud Scheduler oder GitHub Actions).

## Keyword Automationen

- `keyword-cluster.ts` ordnet Keywords zu Personas, intent-basierten Clustern & SERP-Features.
- `local-keywords.ts` erstellt Longtail-Varianten je Region (Input: `localContent.ts`).
- Output wird in `data/high-priority-keywords.json` usw. geschrieben.
- ErgÃ¤nzung: Export als CSV (`--format csv`) fÃ¼r externe Tools.

## Content Pipeline

- Liest `content-plan.json` und `articles.ts`.
- Generiert Briefings (Outline, WDF*IDF, Ziel-Keywords) in `docs/briefings/`.
- Optionaler Hook: Ãœbergabe an Notion via API (`NOTION_TOKEN`).

## Sitemap & Indexierung

- `generate-sitemap.ts` erstellt `public/sitemap.xml` + `sitemap-pages.xml`.
- `ping-search-engines.ts` benachrichtigt Google & Bing nach Deployments.
- Backlog: automatische Einbindung in CI/CD Pipeline.

## Infrastruktur & Scheduling

- AusfÃ¼hrung lokal Ã¼ber `npm run task -- <script>` oder `ts-node`.
- Produktionsbetrieb: 
  - GitHub Actions (`.github/workflows/automation.yml`) â†’ stÃ¼ndliche/daily Jobs.
  - Alternativ Google Cloud Scheduler â†’ Cloud Run Jobs.
- Logging: Scripts schreiben Statusmeldungen in `logs/automation.log` (wenn `LOG_LEVEL` gesetzt).

## Best Practices

1. **Idempotenz:** Scripts stetig so implementieren, dass MehrfachausfÃ¼hrung keine unerwÃ¼nschten Effekte hat.
2. **Konfiguration:** `.env.sample` aktuell halten; neue Variablen dokumentieren.
3. **Monitoring:** Nach Cron-Deploy Slack/Email-Alerts einrichten.
4. **Testing:** Utility-Funktionen mit `vitest` testen (`__tests__/automation/*.test.ts`).
5. **Security:** API Keys nur Ã¼ber Secret Manager, niemals hartkodieren.

## Roadmap

- ðŸ”„ Automatische Berichte (PDF) Ã¼ber Playwright Screenshots + Puppeteer Export.
- ðŸ”„ Integration mit Vertex AI fÃ¼r Keyword-Suggestions (Prompt-Engineering).
- ðŸ”„ Infrastruktur als Terraform-Modul verÃ¶ffentlichen.

---
_Stand: 26.09.2025_